{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMs+cW+JrJLWPVeIIKNkZ2Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobertQShen/Personal-Projects/blob/main/JobSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBXjBc46Ohzu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4bf61b-4f42-4dd5-8299-010f1cae1b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index --quiet\n",
        "!pip install llama-index-llms-groq --quiet\n",
        "!pip install llama-index-core --quiet\n",
        "!pip install llama-index-readers-file --quiet\n",
        "!pip install llama-index-tools-wolfram-alpha --quiet\n",
        "!pip install llama-index-embeddings-huggingface --quiet\n",
        "!pip install 'crewai[tools]' --quiet\n",
        "!pip install -q crewai==0.28.8 crewai_tools==0.1.6 langchain_community==0.0.29\n",
        "!pip install -Uq transformers\n",
        "!pip install -q tokenizers>=0.21,<0.22\n",
        "!pip install numpy==1.25.0\n",
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyWzsTc3EMRo",
        "outputId": "25c1bb4c-3574-4fc6-99b5-068192d9d025",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "embedchain 0.1.113 requires pypdf<5.0.0,>=4.0.1, but you have pypdf 5.4.0 which is incompatible.\n",
            "crewai 0.28.8 requires python-dotenv==1.0.0, but you have python-dotenv 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-cloud-services 0.6.8 requires python-dotenv<2.0.0,>=1.0.1, but you have python-dotenv 1.0.0 which is incompatible.\n",
            "llama-index-readers-file 0.4.7 requires pypdf<6.0.0,>=5.1.0, but you have pypdf 4.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m/bin/bash: line 1: 0.22: No such file or directory\n",
            "Requirement already satisfied: numpy==1.25.0 in /usr/local/lib/python3.11/dist-packages (1.25.0)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.20.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "VKqmwLq3EUCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Define your API key here\n",
        "groq_api_key = userdata.get('groq_key')\n",
        "\n",
        "def select_model(model_name, temperature=0.1, max_tokens=2000):\n",
        "\n",
        "  # using LLama3 on groq free API\n",
        "  llm = ChatOpenAI(\n",
        "      openai_api_base=\"https://api.groq.com/openai/v1\",\n",
        "      openai_api_key=groq_api_key,\n",
        "      model=model_name,\n",
        "      #model = \"distil-whisper-large-v3-en\",\n",
        "      # model=\"llama3-70b-8192\",\n",
        "      # model=\"deepseek-r1-distill-qwen-32b\",\n",
        "      # model = \"deepseek-r1-distill-llama-70b\",\n",
        "      temperature=temperature,\n",
        "      max_tokens=max_tokens,\n",
        "  )\n",
        "  return llm"
      ],
      "metadata": {
        "id": "N-nUNCtMEUcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('Serper_Key')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openaikey')\n",
        "from crewai import Crew, Agent, Task, Process\n",
        "from crewai_tools import FileReadTool\n",
        "from crewai_tools import SerperDevTool\n",
        "from crewai_tools import DirectoryReadTool"
      ],
      "metadata": {
        "id": "7CYQ6UvBEq9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai_tools import BaseTool\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperTokenizer\n",
        "from groq import Groq\n",
        "\n",
        "class JobRankingTool(BaseTool):\n",
        "    name: str =\"Job Ranking Tool\"\n",
        "    description: str = (\"Given the list of jobs provided by the search_and_ranking_agent, rank the postings based on keywords found.\"\n",
        "    )\n",
        "\n",
        "    def _run(self, job_postings: list, keywords: list) -> list:\n",
        "      \"\"\"\n",
        "    Analyze the retrieved job postings using the following ranking algorithm:\n",
        "    - For each job posting (URL), check for the provided keywords.\n",
        "    - Add 1 point to the job's score each time a keyword is found in the posting URL.\n",
        "    - Rank the job postings based on their total score, with higher scores listed at the top.\n",
        "\n",
        "    Args:\n",
        "        job_postings (list): A list of job posting URLs.\n",
        "        keywords (list): A list of keywords to search for.\n",
        "\n",
        "    Returns:\n",
        "        list: A ranked list of job postings, including their scores.\n",
        "      \"\"\"\n",
        "      ranked_jobs = []\n",
        "\n",
        "      for job in job_postings:\n",
        "          score = 0\n",
        "\n",
        "        # Check for keywords in the job posting URL\n",
        "          if isinstance(job, str):  # Ensure it's a string\n",
        "              for keyword in keywords:\n",
        "                  if keyword.lower() in job.lower():\n",
        "                      score += 1\n",
        "\n",
        "        # Add the job and its score to the ranked list\n",
        "          ranked_jobs.append({\"job\": job, \"score\": score})\n",
        "\n",
        "    # Sort jobs by score in descending order\n",
        "      ranked_jobs.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "\n",
        "      return ranked_jobs\n",
        "\n",
        "class WhisperAgent(Agent):\n",
        "    def __init__(self, model_size=\"small\", role=None, goal=None, backstory=None):\n",
        "        super().__init__(role=role, goal=goal, backstory=backstory)\n",
        "        self.llm=Groq(api_key=userdata.get('groq_key'))\n",
        "        #openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "    def transcribe_audio(self, audio_path):\n",
        "        client = self.llm\n",
        "        audio_file= open(audio_path, \"rb\")\n",
        "\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "        model='whisper-large-v3',\n",
        "        file=audio_file\n",
        "        )\n",
        "\n",
        "        return transcription.text"
      ],
      "metadata": {
        "id": "rwFqOdw_B7m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create more tools later\n",
        "directory_read_tool = DirectoryReadTool(directory='/content/drive/MyDrive/crewAI_docs_updated')\n",
        "file_read_tool = FileReadTool()\n",
        "search_tool = SerperDevTool()\n",
        "ranking_tool= JobRankingTool()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MeuRemfJE3CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_processing_agent = Agent(\n",
        "    role=\"Job Query Processor and Comparison Agent\",\n",
        "    goal=(\n",
        "        \"Process and clean user job queries, translating them into a structured format. \"\n",
        "        \"Compare and rank various job options based on similarity, relevance, or importance to the user's requirements.\"\n",
        "    ),\n",
        "    backstory=(\n",
        "        \"An expert in job data processing and comparison, you are skilled at interpreting and refining user queries. \"\n",
        "        \"Your mission is to ensure the user's search intent is accurately captured and translated into a structured format. \"\n",
        "        \"Using advanced tools, you clean and preprocess the query, then compare and rank job options to present the most relevant results. \"\n",
        "        \"Your goal is to deliver precise and user-friendly outputs that align closely with the user's preferences and constraints.\"\n",
        "    ),\n",
        "    llm=select_model(model_name=\"llama-3.1-8b-instant\", max_tokens=2000),\n",
        "    allow_delegation=False,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "mYjt0kJIGyun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_and_ranking_agent = Agent(\n",
        "    role=\"Search and Ranking Agent\",\n",
        "    goal=(\"Compare and rank job options based on similarity, relevance, or importance to the user's requirements \"\n",
        "    \"as given by the user query generated by the query_processing_agent.\"\n",
        "    ),\n",
        "    backstory=(\n",
        "        \"An expert in comparing and ranking job postings, you analyze job data to find the best matches for the user. \"\n",
        "        \"You use the given search and rank tools to rank options based on user preferences and constraints.\"\n",
        "    ),\n",
        "    tools=[search_tool],\n",
        "    llm=select_model(model_name = \"llama-3.1-8b-instant\", max_tokens = 2000),\n",
        "    allow_delegation=False,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "H3DPuxM2Uki6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_agent = Agent(\n",
        "    role=\"Response Agent\",\n",
        "    goal=(\"Given the list of jobs provided by the search_and_ranking_agent, communicate the options to the users in a polite and professional manner.\"\n",
        "    ),\n",
        "    backstory=(\n",
        "        \"You are a polite and helpful communicator that clearly gives the user the most relevant job postings that matches the user's requirements.\"\n",
        "    ),\n",
        "    llm=select_model(model_name = \"llama-3.1-8b-instant\", max_tokens = 2000),\n",
        "    allow_delegation=False,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "pnk4RSL8jF9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_query = Task(\n",
        "    description=(\n",
        "        \"Process the user query to extract and clean details like location, job title, \"\n",
        "        \"salary, and company. The user query is: {user_query}. Return a structured query.\"\n",
        "        \"If user query is not asking for a job search, stop processing.\"\n",
        "    ),\n",
        "    expected_output=('A structured query in JSON format.'),\n",
        "\n",
        "    agent = query_processing_agent,\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "CR3iAM_1UAMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieve_and_rank = Task(\n",
        "    description=(\n",
        "        \"1. Retrieve job postings using the structured query provided by query_processing_agent using the search tool **once**. Must include at least 5 key words: location, job title, company, salary, and salary range from user query. \"\n",
        "        \"2. After retrieving the results, **stop searching** and do not perform any additional searches. \"\n",
        "        \"3. Use the ranking_tool to rank the job postings.\"\n",
        "        \"4. Return the ranked list and **Stop task**.\"\n",
        "    ),\n",
        "    expected_output=(\n",
        "        \"A ranked list of job postings and URLs, sorted by their score. If scores are 0, list alphabetically.\"\n",
        "    ),\n",
        "    tools=[search_tool, ranking_tool],\n",
        "    agent = search_and_ranking_agent,\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "XGnufluXyeJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_final_response = Task(\n",
        "    description=(\n",
        "        \"Generate a user-friendly response based on the ranked job postings created by search_and_ranking_agent.\"\n",
        "        \"Break the Json list apart and turn it into a numbered list in a string format.\"\n",
        "        \"The response should be clear, concise, and easy to understand.\"\n",
        "        \"After finishing, **Stop task**.\"\n",
        "    ),\n",
        "    expected_output=(\"A list of the top ranked and msot relevant jobs with the location, company, job title, stated salary, and URL link.\"),\n",
        "    agent = response_agent,\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "MsaIVexQaXnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crew = Crew(\n",
        "    agents=[query_processing_agent,\n",
        "            search_and_ranking_agent, response_agent],\n",
        "\n",
        "    tasks=[process_query,\n",
        "           retrieve_and_rank, generate_final_response],\n",
        "\n",
        "    verbose=2,\n",
        "    # llm=select_model(model_name = \"llama3-70b-8192\")\n",
        "\t#  memory=True\n",
        ")\n",
        "role=\"Conversational Input Agent\"\n",
        "goal=\"Capture real-time voice input from the user, transcribe it into text, and pass the transcribed text to other agents for further processing.\"\n",
        "backstory=\"You are a highly skilled voice input processor. Your expertise lies in capturing audio, transcribing it accurately, and ensuring the transcribed text is ready for downstream tasks.\"\n",
        "\n",
        "class CentralController:\n",
        "    def __init__(self, crew):\n",
        "        \"\"\"\n",
        "        Initializes the Central Controller with the CrewAI crew.\n",
        "        Parameters:\n",
        "            crew: The CrewAI crew managing the agents and tasks.\n",
        "        \"\"\"\n",
        "        self.crew = crew\n",
        "        self.whisper_agent = WhisperAgent(role=\"assistant\", goal=\"transcribe\", backstory=\"none\")\n",
        "\n",
        "    def handle_user_query(self, user_query):\n",
        "        \"\"\"\n",
        "         Handles the entire workflow for a user query using the CrewAI crew.\n",
        "        Parameters:\n",
        "            user_query (dict): The raw user query.\n",
        "        Returns:\n",
        "            str: The final response to the user.\n",
        "        \"\"\"\n",
        "        print(\"Handling user query...\")\n",
        "\n",
        "        result = self.crew.kickoff(inputs={\"user_query\": user_query})\n",
        "        return result\n",
        "\n",
        "    def handle_voice_query(self, audio_path):\n",
        "        \"\"\"\n",
        "         Handles the entire workflow for a user query using the CrewAI crew.\n",
        "        Parameters:\n",
        "            user_query (dict): The raw user query.\n",
        "        Returns:\n",
        "            str: The final response to the user.\n",
        "        \"\"\"\n",
        "        print(\"Handling voice query...\")\n",
        "        transcribed_text = self.whisper_agent.transcribe_audio(audio_path)\n",
        "\n",
        "        result = self.crew.kickoff(inputs={\"user_query\": transcribed_text})\n",
        "        return result"
      ],
      "metadata": {
        "id": "liw08svWvr9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok python-multipart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tSQtg4V5Q4PB",
        "outputId": "7ef6acb7-7d71-482c-bd5e-a94b1aea5b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: python-multipart, pyngrok\n",
            "Successfully installed pyngrok-7.2.3 python-multipart-0.0.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import time\n",
        "import os\n",
        "from fastapi import FastAPI, Request, Form, File, UploadFile\n",
        "from fastapi.responses import HTMLResponse\n",
        "from fastapi.templating import Jinja2Templates\n",
        "from typing import Optional\n",
        "\n",
        "central_controller = CentralController(crew)\n",
        "\n",
        "app = FastAPI()\n",
        "templates = Jinja2Templates(directory=\"/content/drive/My Drive/Colab Notebooks/templates\")\n",
        "\n",
        "# Endpoint to handle voice queries\n",
        "@app.post(\"/search_voice\", response_class=HTMLResponse)\n",
        "async def search_jobs_voice(request: Request, voice_file: UploadFile = File(...)):\n",
        "    # Save the uploaded file temporarily\n",
        "    temp_file_path = f\"temp_{voice_file.filename}\"\n",
        "    with open(temp_file_path, \"wb\") as buffer:\n",
        "        buffer.write(voice_file.file.read())\n",
        "\n",
        "    # Pass the voice file to the Voice Capture Agent for transcription\n",
        "    results = central_controller.handle_voice_query(temp_file_path)\n",
        "\n",
        "    # Clean up the temporary file\n",
        "    os.remove(temp_file_path)\n",
        "\n",
        "    return templates.TemplateResponse(\"index.html\", {\"request\": request, \"results\": results})\n",
        "\n",
        "@app.post(\"/search\", response_class=HTMLResponse)\n",
        "async def search_jobs(request: Request, query: str = Form(...)):\n",
        "    results = central_controller.handle_user_query({\"text\": query})\n",
        "    return templates.TemplateResponse(\"index.html\", {\"request\": request, \"results\": results})\n",
        "\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "async def read_root(request: Request):\n",
        "    return templates.TemplateResponse(\"index.html\", {\"request\": request, \"results\": None})\n",
        "\n",
        "def run_uvicorn():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "# Set your ngrok authtoken\n",
        "ngrok.set_auth_token(\"2tm4fEcmxh7QMddWUxnhD000qUW_EHt7ynH8dwbekgaomqv1\")  # Replace with your actual authtoken\n",
        "\n",
        "# Close all existing tunnels to avoid hitting the limit\n",
        "ngrok.kill()\n",
        "\n",
        "# Set up ngrok\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# Start Uvicorn in a background thread\n",
        "uvicorn_thread = threading.Thread(target=run_uvicorn, daemon=True)\n",
        "uvicorn_thread.start()\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Shutting down...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcJxl8uHWLHz",
        "outputId": "14ee4a86-da8c-4302-92a6-a333a1951829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://118f-34-168-168-204.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [2520]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2607:fea8:1f9e:9920:7dde:6ec3:2e3d:c856:0 - \"GET / HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 112, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 714, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 734, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 73, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 301, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-16-b2298c810308>\", line 39, in read_root\n",
            "    return templates.TemplateResponse(\"index.html\", {\"request\": request, \"results\": None})\n",
            "           ^^^^^^^^^\n",
            "NameError: name 'templates' is not defined\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2607:fea8:1f9e:9920:7dde:6ec3:2e3d:c856:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "Shutting down...\n"
          ]
        }
      ]
    }
  ]
}